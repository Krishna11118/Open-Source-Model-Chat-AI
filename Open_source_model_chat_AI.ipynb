# %%
# !pip install jupyter-dash
# !npm install -q localtunnel
!pip install -q streamlit
!pip install pyngrok

# %%
!pip install ollama PyPDF2 textract==1.6.3 pandas python-docx

# %%
import os
from threading import Thread
from pyngrok import ngrok

# %%
!pip install flask flask-ngrok

# %%
!pip install langchain
!pip install langchain-core
!pip install langchain-community
!pip install colab-xterm

# %% [markdown]
# curl -fsSL https://ollama.com/install.sh | sh

# %%
%load_ext colabxterm
%xterm

# %%
from threading import Thread
import os
def run_ollama():
  os.system('ollama serve')
#To run streamlit in background
thread2 = Thread(target=run_ollama)
thread2.start()

# %%
!ollama pull llama3.1

# %%
!ollama list

# %%
import ollama
response = ollama.chat(model="llama3.1", messages=[{'role': 'user', 'content': 'Hello!'}])
print(response)

# %%
print(type(response))

# %%
%%writefile app.py
import streamlit as st
import ollama
import PyPDF2
import textract
import pandas as pd
import io
from docx import Document

# Function to extract data or perform statistical analysis from documents
def extract_data_from_document(uploaded_file):
    try:
        # Convert uploaded file to bytes
        file_bytes = uploaded_file.getvalue()
        file_name = uploaded_file.name

        if uploaded_file.type == 'application/pdf':
            # Extract text from PDF
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))
            text = "\n".join([page.extract_text() for page in pdf_reader.pages])
            return {"type": "text", "content": text}

        elif uploaded_file.type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':
            # Extract text from DOCX using python-docx
            document = Document(io.BytesIO(file_bytes))
            text = "\n".join([para.text for para in document.paragraphs])
            return {"type": "text", "content": text}

        elif uploaded_file.type == 'text/plain':
            # Extract text from TXT
            text = file_bytes.decode('utf-8')
            return {"type": "text", "content": text}

        elif uploaded_file.type in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
            # Excel file - use pandas to extract all rows as JSON and perform statistical analysis
            df = pd.read_excel(io.BytesIO(file_bytes))
            json_data = df.to_json(orient="records")  # Convert to JSON format
            stats = perform_statistical_analysis(df)  # Get basic statistical analysis
            return {"type": "json", "content": json_data, "dataframe": df, "statistics": stats}

        elif uploaded_file.type == 'text/csv':
            # CSV file - use pandas to extract all rows as JSON and perform statistical analysis
            df = pd.read_csv(io.BytesIO(file_bytes))
            json_data = df.to_json(orient="records")  # Convert to JSON format
            stats = perform_statistical_analysis(df)  # Get basic statistical analysis
            return {"type": "json", "content": json_data, "dataframe": df, "statistics": stats}

        else:
            # Generic handler for other file types using textract
            text = textract.process(input_bytes=file_bytes, filename=file_name).decode('utf-8')  # Pass both bytes and filename
            return {"type": "text", "content": text}

    except Exception as e:
        st.error(f"Error extracting data: {e}")
        return {"type": "error", "content": ""}


# Function to perform basic statistical analysis on a DataFrame
def perform_statistical_analysis(df):
    try:
        numerical_stats = df.describe().T  # Transpose for better readability
        stats_dict = numerical_stats.to_dict()  # Convert to dictionary for JSON-like structure
        return stats_dict
    except Exception as e:
        st.error(f"Error performing statistical analysis: {e}")
        return {}

def chat_with_document():
    st.title("ðŸ’¬ Chat with Document")

    # Modify file uploader to support CSV and Excel as well
    uploaded_file = st.file_uploader(
        "Upload a document",
        type=['pdf', 'docx', 'txt', 'csv', 'xlsx'],
        help="Upload PDF, DOCX, TXT, CSV, or Excel files"
    )

    if "doc_messages" not in st.session_state:
        st.session_state.doc_messages = []

    if uploaded_file is not None:
        document_data = extract_data_from_document(uploaded_file)

        st.subheader("Ask questions about the document")

        # Add a button to preview the document
        with st.expander("Document Preview"):
            if document_data["type"] == "text":
                st.text(document_data["content"][:1500] + "..." if len(document_data["content"]) > 1500 else document_data["content"])
            elif document_data["type"] == "json":
                st.json(document_data["content"])
                if "dataframe" in document_data:
                    st.dataframe(document_data["dataframe"])  # Display the table in the preview
                    st.subheader("Basic Statistical Analysis")
                    st.dataframe(document_data["statistics"])  # Display statistical analysis

        # Display chat history (previous messages)
        for message in st.session_state.doc_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("What would you like to know about this document?"):
            st.session_state.doc_messages.append({"role": "user", "content": prompt})

            with st.chat_message("user"):
                st.markdown(prompt)

            # Combine the document context with the conversation history
            document_context = f"Document Context:\n\n"

            if document_data["type"] == "json":
                document_context += f"Data in JSON format:\n{document_data['content']}\n\n"
                document_context += f"Statistical Analysis:\n{document_data['statistics']}\n\n"
            else:
                document_context += f"{document_data['content']}\n\n"

            full_context = f"{document_context}\nConversation History:\n"
            for message in st.session_state.doc_messages:
                full_context += f"{message['role'].capitalize()}: {message['content']}\n"
            full_context += f"\nUser Query: {prompt}"

            # Use ollama.chat to interact with the model
            stream = ollama.chat(
                model='llama3.1',
                messages=[{'role': 'system', 'content': 'You are a helpful assistant that answers questions based on the provided document context.'},
                          {'role': 'user', 'content': full_context}],
                stream=True
            )

            # Create a placeholder to update the message incrementally
            response_placeholder = st.empty()
            response = ""

            # Stream the response as it's generated
            for chunk in stream:
                chunk_text = chunk.get("message", {}).get("content", "")
                if chunk_text.strip():  # Only add non-empty chunks
                    response += chunk_text
                    response_placeholder.markdown(response)  # Update the placeholder with the current response
            response_placeholder.empty()
            # Once streaming is complete, add the final response to the chat history (only once)
            with st.chat_message("assistant"):
                st.markdown(response)

            # Add assistant's response to chat history (only after streaming is complete)
            st.session_state.doc_messages.append({"role": "assistant", "content": response})


    else:
        st.info("Please upload a document to start chatting")


# Main function with app mode selector
def main():
    st.sidebar.title("Document Chat")
    chat_with_document()


if __name__ == "__main__":
    main()

# %%
def run_streamlit():
  os.system('streamlit run app.py --server.port 8501')

# %%
#To run streamlit in background
thread = Thread(target=run_streamlit)
thread.start()

# %%
#Creating tunnel
import pyngrok
from pyngrok import ngrok
ngrok.set_auth_token("2rkrrdzN7OJOLhVfbh0cl1O3oNp_61HRToAe4dvDcZFBKAsQh")
public_url = ngrok.connect('localhost:8501', bind_tls=True)
print(public_url)

# %%
from pyngrok import ngrok

# Open a HTTP tunnel on the default port 80
http_tunnel = ngrok.connect(8501, "http")
print(f"Your ngrok public URL is: {http_tunnel.public_url}")


# %%
from pyngrok import ngrok

# Get the active tunnel by its public URL
tunnel = ngrok.get_tunnel("https://e944-34-90-255-216.ngrok-free.app")

# Kill the tunnel
if tunnel:
  ngrok.kill()  # No need to pass any arguments when killing using tunnel object
else:
  print("Tunnel not found")

# %%
ngrok.kill("https://e944-34-90-255-216.ngrok-free.app" )

# %%
!ollama list

# %%
response.keys()

# %%
!curl https://loca.lt/mytunnelpassword

# %%
!wget -q -O - https://loca.lt/mytunnelpassword

# %%
!wget -q -O - https://loca.lt/mytunnelpassword

# %%
from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok
import subprocess
import json
import time
import logging

app = Flask(__name__)
run_with_ngrok(app)  # Start the Flask app with ngrok

logger = logging.getLogger(__name__)

@app.route('/search_images', methods=['POST'])
def search_images():
    try:
        data = request.json
        query = data.get('query', '')
        metadata = data.get('metadata', '')

        # Prepare the prompt for Llama 3.2 Vision model
        prompt = f"""Given the following image metadata: {metadata}, and the user query: "{query}", return only a list of image file names that best match the search criteria"""
        logger.info(f'Prompt: {prompt}')

        start_time = time.time()

        # Run Ollama using subprocess
        process = subprocess.Popen(
            ['ollama', 'run', 'llama3.1', prompt],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        try:
            stdout, stderr = process.communicate(timeout=1200)  # Increased timeout
            end_time = time.time()
            elapsed_time = end_time - start_time
            logger.info(f'Ollama Response Time: {elapsed_time} seconds')
            logger.info(f'stdout: {stdout}')
            logger.info(f'stderr: {stderr}')

            if process.returncode == 0:
                try:
                    # Attempt to parse the output as a JSON list
                    image_names = json.loads(stdout)

                    # Ensure it's a list
                    if not isinstance(image_names, list):
                        image_names = [stdout.strip()]
                except json.JSONDecodeError as e:
                    logger.error(f'JSONDecodeError error: {e}')
                    image_names = [stdout.strip()]

                # Clean up image names: remove quotes and whitespace
                cleaned_image_names = [name.strip().replace('"', '') for name in image_names]
                return jsonify({'image_names': cleaned_image_names})
            else:
                logger.error(f'Ollama Error: {stderr}')
                return jsonify({'error': f'Ollama error: {stderr}'}), 500

        except subprocess.TimeoutExpired:
            logger.error('Ollama process timed out.')
            return jsonify({'error': 'Ollama process timed out'}), 500

        except Exception as e:
            logger.error(f'Error during ollama process: {e}')
            return jsonify({'error': f'An error occurred during ollama process {e}'}), 500

    except Exception as e:
        logger.error(f'An unexpected error occurred: {e}')
        return jsonify({'error': f'An unexpected error occurred: {e}'}), 500

if __name__ == "__main__":
    app.run()


# %%



